{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XcUPQ9CkLdM"
   },
   "outputs": [],
   "source": [
    "# ANN\n",
    "# dataset description :-\n",
    "# fictional dataset of a bank having 10,000 customers and therefore, 10,000 rows\n",
    "# churn rate (rate at which people are leaving the bank) - we need to predict which of the customers are\n",
    "# at highest risk of leaving - classification problem\n",
    "# columns correspond to diff features of the customers like gender, country, num of products, age, balance, etc\n",
    "# last column is whether the customer exited or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9zrwgXspMUR"
   },
   "outputs": [],
   "source": [
    "# MADE IN GOOGLE COLABORATORY\n",
    "# final model saved as \"FINAL_CLASSIFIER.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DYl6MfKkLdU"
   },
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NpoUSWSvkLdW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sNIAZj5kLdb"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "asGsueKDkLde",
    "outputId": "545f79c6-b3b7-4b0f-d846-3c2fffc76d86"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  ...  IsActiveMember EstimatedSalary Exited\n",
       "0          1    15634602  Hargrave  ...               1       101348.88      1\n",
       "1          2    15647311      Hill  ...               1       112542.58      0\n",
       "2          3    15619304      Onio  ...               0       113931.57      1\n",
       "3          4    15701354      Boni  ...               0        93826.63      0\n",
       "4          5    15737888  Mitchell  ...               1        79084.10      0\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LpPuzfD9kLdl",
    "outputId": "47b6d429-d6e5-4ee4-afd5-0cc4c77c35b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10), (10000,))"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, 3:13].values # rowNumber, customerId and surname play NO role in churn rate \n",
    "y = dataset.iloc[:, 13].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "fapcYtmrkLdt",
    "outputId": "7964717a-dddd-48c0-9322-44941900d4d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NpKk5mhmkLdx",
    "outputId": "d97fa459-607b-4543-cebb-59a81e9a5836"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lryj-JG6kLd1",
    "outputId": "068b1bb4-c568-41dd-cf15-cac1e26be4c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0] # y contains values 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skUeCKY-kLd6"
   },
   "outputs": [],
   "source": [
    "# encode categorical variables (country and gender) before splitting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46Qc_aQGkLd-"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#encode country\n",
    "encoder1 = LabelEncoder()\n",
    "X[:,1] = encoder1.fit_transform(X[:,1])\n",
    "\n",
    "# encode gender - we'll not onehot encode this cz it only has 2 categories - since we'll be removing one of the columns to avoid \n",
    "# dummy variable trap, it'll be no use to one hot encode this\n",
    "encoder2 = LabelEncoder()\n",
    "X[:,2] = encoder2.fit_transform(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "7Cfoi3oSkLeC",
    "outputId": "b9b4dbd7-8d18-4826-f103-d361d2e02fb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 6.1900000e+02,\n",
       "        0.0000000e+00, 4.2000000e+01, 2.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0134888e+05]), (12,))"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode country\n",
    "ohe = OneHotEncoder(categorical_features = [1])\n",
    "X = ohe.fit_transform(X).toarray()\n",
    "X[0], X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "MsNjKsthkLeI",
    "outputId": "7a9184b8-5a13-4b60-be57-af9df5f41f10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0000000e+00, 0.0000000e+00, 6.1900000e+02, 0.0000000e+00,\n",
       "        4.2000000e+01, 2.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0134888e+05]), (11,))"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove dummy variable we get after ohe\n",
    "X = X[:,1:] # will remove 1st column\n",
    "X[0], X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Cc0zZG8kLeO",
    "outputId": "fbedef32-ece1-4d2c-deee-23078a4fdc23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 11), (10000,))"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxYeLCh0kLeS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "llPd7NN0kLeX",
    "outputId": "2868aca8-0c29-406d-af5c-c00bee93e723"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 11), (8000,))"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hy44sQDqkLef",
    "outputId": "a68e5209-0ed9-463e-a36e-eaacd116e380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 11), (2000,))"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRWjE-mukLel"
   },
   "outputs": [],
   "source": [
    "# do feature scaling - we dont want to have one independent var dominating another one\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "SrWDUP-gkLeo",
    "outputId": "4970fbb6-a6b2-464a-d3e8-5378ef9dcf84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5698444 ,  1.74309049,  0.16958176, -1.09168714, -0.46460796,\n",
       "        0.00666099, -1.21571749,  0.8095029 ,  0.64259497, -1.03227043,\n",
       "        1.10643166])"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "6uMKY6pDkLev",
    "outputId": "e6b68648-806e-409c-cca1-0fea08719439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.75486502, -0.57369368, -0.55204276, -1.09168714, -0.36890377,\n",
       "        1.04473698,  0.8793029 , -0.92159124,  0.64259497,  0.9687384 ,\n",
       "        1.61085707])"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pllm1_cjkLe3"
   },
   "source": [
    "BUILDING THE ANN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8PILGU1kLe4"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "DMS0T4OckLe9",
    "outputId": "94a6186c-b196-4334-b295-71e1b5de528a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 07:55:27.237307 139705763219328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential() # initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "9LfZymYFkLfA",
    "outputId": "b8d1b594-5204-4694-9f13-79fbb93440ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 07:55:27.299245 139705763219328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0728 07:55:27.323942 139705763219328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add input layer and first hidden layer\n",
    "# no of nodes in input layer = 11 ( X has 11 features)\n",
    "# no of nodes in output layer = 1 ( coresponding to y value)\n",
    "# choose 6 for no of nodes in hidden layer ( average of 11 and 1)\n",
    "classifier.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu', input_dim = 11)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNiXcPXkkLfD"
   },
   "outputs": [],
   "source": [
    "# add second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOkq58c7kLfF"
   },
   "outputs": [],
   "source": [
    "# add output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer='uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "colab_type": "code",
    "id": "RYTPWxsSkLfI",
    "outputId": "fdf36670-4110-4501-a0c3-b7529273ba7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 07:55:27.428335 139705763219328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0728 07:55:27.457488 139705763219328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0728 07:55:27.466404 139705763219328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# compile the ann\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# optimizer - the algo we want to use to find the optimal set of weights in the nn - adam optimizer is a v efficient type\n",
    "# of stochastic grad optimization\n",
    "# loss - the loss function used within the sgd adam algo \n",
    "# binary_crossentropy - used with binary classification\n",
    "# categorical_crossentropy - used with multiclass classification\n",
    "# metrics - metrics used by algo after every iteration to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "V5GMiXtlkLfN",
    "outputId": "b0d31023-0a98-460b-e84e-813340f61a57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 07:55:27.768816 139705763219328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.4869 - acc: 0.7969\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 4s 451us/step - loss: 0.4109 - acc: 0.8217\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 0.3958 - acc: 0.8281\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 3s 425us/step - loss: 0.3867 - acc: 0.8299\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 4s 443us/step - loss: 0.3802 - acc: 0.8331\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 4s 448us/step - loss: 0.3752 - acc: 0.8420\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 4s 476us/step - loss: 0.3713 - acc: 0.8471\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 0.3687 - acc: 0.8494\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 0.3666 - acc: 0.8499\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 4s 469us/step - loss: 0.3639 - acc: 0.8516\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 4s 474us/step - loss: 0.3617 - acc: 0.8539\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 4s 493us/step - loss: 0.3604 - acc: 0.8531\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 0.3587 - acc: 0.8535\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 0.3571 - acc: 0.8535\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 4s 440us/step - loss: 0.3561 - acc: 0.8555\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 4s 441us/step - loss: 0.3547 - acc: 0.8539\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 4s 449us/step - loss: 0.3538 - acc: 0.8550\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 0.3530 - acc: 0.8555\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 3s 437us/step - loss: 0.3525 - acc: 0.8547\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 4s 451us/step - loss: 0.3518 - acc: 0.8556\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 3s 424us/step - loss: 0.3520 - acc: 0.8567\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 0.3512 - acc: 0.8585\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.3501 - acc: 0.8577\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 4s 463us/step - loss: 0.3501 - acc: 0.8579\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 4s 451us/step - loss: 0.3504 - acc: 0.8564\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 3s 432us/step - loss: 0.3489 - acc: 0.8592\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 3s 431us/step - loss: 0.3492 - acc: 0.8574\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 4s 466us/step - loss: 0.3486 - acc: 0.8571\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 4s 469us/step - loss: 0.3485 - acc: 0.8585\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 4s 449us/step - loss: 0.3477 - acc: 0.8584\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.3480 - acc: 0.8590\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 4s 463us/step - loss: 0.3461 - acc: 0.8585\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 0.3459 - acc: 0.8587\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 0.3453 - acc: 0.8591\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 4s 438us/step - loss: 0.3448 - acc: 0.8590\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 4s 442us/step - loss: 0.3447 - acc: 0.8600\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 4s 467us/step - loss: 0.3451 - acc: 0.8587\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 0.3447 - acc: 0.8584\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 3s 435us/step - loss: 0.3451 - acc: 0.8577\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 0.3440 - acc: 0.8600\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 3s 434us/step - loss: 0.3431 - acc: 0.8586\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 4s 467us/step - loss: 0.3437 - acc: 0.8582\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 4s 475us/step - loss: 0.3435 - acc: 0.8582\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 4s 450us/step - loss: 0.3433 - acc: 0.8597\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 4s 466us/step - loss: 0.3446 - acc: 0.8576\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 4s 502us/step - loss: 0.3434 - acc: 0.8602\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 4s 474us/step - loss: 0.3436 - acc: 0.8600\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 4s 454us/step - loss: 0.3432 - acc: 0.8589\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 4s 463us/step - loss: 0.3430 - acc: 0.8581\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 4s 439us/step - loss: 0.3431 - acc: 0.8577\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 4s 475us/step - loss: 0.3433 - acc: 0.8592\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.3429 - acc: 0.8591\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 4s 484us/step - loss: 0.3426 - acc: 0.8590\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 4s 465us/step - loss: 0.3427 - acc: 0.8607\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 4s 452us/step - loss: 0.3425 - acc: 0.8590\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 4s 448us/step - loss: 0.3419 - acc: 0.8604\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 3s 433us/step - loss: 0.3425 - acc: 0.8585\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 0.3429 - acc: 0.8607\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 0.3418 - acc: 0.8595\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 0.3423 - acc: 0.8609\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 3s 437us/step - loss: 0.3412 - acc: 0.8576\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 4s 448us/step - loss: 0.3418 - acc: 0.8601\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 4s 474us/step - loss: 0.3419 - acc: 0.8602\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 4s 472us/step - loss: 0.3413 - acc: 0.8596\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 0.3410 - acc: 0.8607\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 4s 476us/step - loss: 0.3409 - acc: 0.8607\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 4s 450us/step - loss: 0.3418 - acc: 0.8580\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 0.3406 - acc: 0.8625\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 4s 448us/step - loss: 0.3416 - acc: 0.8589\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 4s 446us/step - loss: 0.3406 - acc: 0.8602\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 3s 436us/step - loss: 0.3397 - acc: 0.8605\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 4s 491us/step - loss: 0.3406 - acc: 0.8604\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 4s 477us/step - loss: 0.3408 - acc: 0.8614\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 0.3401 - acc: 0.8607\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 3s 422us/step - loss: 0.3396 - acc: 0.8582\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 4s 468us/step - loss: 0.3396 - acc: 0.8596\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 0.3400 - acc: 0.8614\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 4s 440us/step - loss: 0.3397 - acc: 0.8606\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 4s 439us/step - loss: 0.3396 - acc: 0.8620\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 0.3396 - acc: 0.8595\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 4s 439us/step - loss: 0.3399 - acc: 0.8611\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 3s 436us/step - loss: 0.3396 - acc: 0.8579\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 3s 423us/step - loss: 0.3403 - acc: 0.8619\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.3378 - acc: 0.8630\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 3s 421us/step - loss: 0.3388 - acc: 0.8614\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 0.3380 - acc: 0.8632\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 3s 427us/step - loss: 0.3387 - acc: 0.8617\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 3s 433us/step - loss: 0.3388 - acc: 0.8600\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 3s 429us/step - loss: 0.3382 - acc: 0.8590\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 3s 428us/step - loss: 0.3387 - acc: 0.8614\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 4s 477us/step - loss: 0.3388 - acc: 0.8580\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 0.3383 - acc: 0.8589\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 4s 447us/step - loss: 0.3390 - acc: 0.8591\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 4s 447us/step - loss: 0.3383 - acc: 0.8590\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 4s 449us/step - loss: 0.3371 - acc: 0.8605\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.3381 - acc: 0.8612\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 4s 481us/step - loss: 0.3377 - acc: 0.8596\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 4s 484us/step - loss: 0.3379 - acc: 0.8585\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 4s 510us/step - loss: 0.3381 - acc: 0.8617\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 0.3380 - acc: 0.8594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f75408e10>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the ann to the training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvlE5793VjSl"
   },
   "outputs": [],
   "source": [
    "# save the model so we dont have to train again and again\n",
    "import pickle\n",
    "classifier.save(\"classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojsg3R-mkLfd"
   },
   "outputs": [],
   "source": [
    "# accuracy converging at about 86%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t28rwP-QkLfj"
   },
   "source": [
    "MAKING PREDICTIONS AND EVALUATING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIC4SxCAkLfk"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "hmXP9RClkLfs",
    "outputId": "918e78f4-e619-41cb-d092-e83ac95e0fa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19588643],\n",
       "       [0.32081297],\n",
       "       [0.09965912],\n",
       "       ...,\n",
       "       [0.21925613],\n",
       "       [0.11504003],\n",
       "       [0.27958232]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred # gives probabilities that a customer will leave the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "uq54EBnqkLfy",
    "outputId": "60cccbce-85df-4d04-bec7-62e4c0e7278a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        ...,\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]]), (2000, 1))"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert y_pred in the form true/false\n",
    "y_pred = (y_pred > 0.5) # return true if y_pred > 0.5\n",
    "y_pred, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "nmHD8Q0ekLf1",
    "outputId": "9977929f-6b83-4c52-fe2c-68a93efbb904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1507,   88],\n",
       "       [ 196,  209]])"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-GD_BUNikLf5",
    "outputId": "012aa8e8-0afc-4e3e-c858-22ab9a66bc12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.845"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of 2000 observations, we get 1544+146 correct predictions and 51+259 wrong predictions\n",
    "# accuracy = (no of correct preds)/(no of total preds)\n",
    "accuracy = (1544+146)/2000\n",
    "accuracy # on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMZ8iCXDkLf_"
   },
   "outputs": [],
   "source": [
    "# results\n",
    "# training set accuracy = ~86%\n",
    "# test set accuracy = ~84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rOsAskcFkLgE"
   },
   "source": [
    "PREDICTING RESULTS FOR NEW TEST POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZJWxEGJkLgF"
   },
   "outputs": [],
   "source": [
    "new_test_point = np.array([[0,0,600,1,40,3,60000,2,1,1,50000]])\n",
    "new_test_point = sc.transform(new_test_point) # scaling\n",
    "#new_test_point = new_test_point.reshape(-1,1)\n",
    "new_pred = classifier.predict(new_test_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YoExba1YkLgI",
    "outputId": "963a44a9-572e-4c78-8d60-a9907737a78a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03879036]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A30uIpg1kLgL",
    "outputId": "4df4cf01-00f2-40cb-9511-6fb8245113fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = (new_pred > 0.5)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yzpcNT6kkLgS"
   },
   "outputs": [],
   "source": [
    "# the new customer doesnt leave the bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSujAFfmkLgW"
   },
   "source": [
    "EVALUATING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiqOOX0ckLgZ"
   },
   "outputs": [],
   "source": [
    "# apply kfold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0h57caGVkLgb"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# we are training the model using keras and using kfold cross validation which belongs to scklearn - KerasClassifier is a wrapper\n",
    "# that helps us combine these 2 libraries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTmu65tdkLgd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHCt4Ib2kLgh"
   },
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu', input_dim = 11)) \n",
    "    clf.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu')) \n",
    "    clf.add(Dense(units = 1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "    clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOAJC07ekLgj"
   },
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD-4S5_vkLgl"
   },
   "outputs": [],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 2, n_jobs  =-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8EybkKbfkLgy"
   },
   "outputs": [],
   "source": [
    "accuracies2 = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs  =-1)\n",
    "# n_jobs = -1 means all CPUs are used\n",
    "# cv - 10 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "woWNLyF4kLg1",
    "outputId": "f7304453-33f9-40ef-aa07-f3b35588f529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85749999, 0.83749999, 0.88      , 0.82625   , 0.86999999,\n",
       "       0.83      , 0.83375   , 0.85875   , 0.81125   , 0.84749999])"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fb7cfRzmkLg6",
    "outputId": "4e90f0a9-e92a-4434-88b5-170701f5aea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452499948441983"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = accuracies2.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zr0dXpxnkLg-",
    "outputId": "1df8d67d-4595-4877-bb87-600187201233"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020246912674531055"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance = accuracies2.std()\n",
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nx8PYRGU0Zd7"
   },
   "outputs": [],
   "source": [
    "# save the accuracies array\n",
    "np.save('10_fold_cross_val_accuracies', accuracies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMHjCKs_1cOl"
   },
   "outputs": [],
   "source": [
    "# load the accuracies object\n",
    "accuracies2 = np.load('10_fold_cross_val_accuracies.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTXENE42kLhC"
   },
   "outputs": [],
   "source": [
    "# no need to apply dropout regularization since all accuracies in cross validation quite close to each other - no \n",
    "# overfitting is there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmyuvfwnkLhF"
   },
   "source": [
    "TUNING THE ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVUppNk0kLhG"
   },
   "outputs": [],
   "source": [
    "# apply parameter tuning\n",
    "# grid search - will find out best values for the hyperparameters by trying out different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKBOFL3lkLhI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5et04nNE2wB"
   },
   "outputs": [],
   "source": [
    "def build_classifier_gs():\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu', input_dim = 11)) \n",
    "    clf.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu')) \n",
    "    clf.add(Dense(units = 1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "    clf.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2xSjq802Lwm"
   },
   "outputs": [],
   "source": [
    "clf_tune = KerasClassifier(build_fn = build_classifier_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKXiljKq0OYf"
   },
   "outputs": [],
   "source": [
    "def build_classifier_grid_search():\n",
    "  # dictionary consisting of different hyperparameters and their values we want to test out - grid search will return the best combinations out of these values\n",
    "  parameters = {'batch_size' : [10, 25, 32],\n",
    "             'nb_epoch' : [100,250, 500]}\n",
    "  grid_search = GridSearchCV(estimator = clf_tune, param_grid = parameters, scoring = 'accuracy', cv = 10)\n",
    "  return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "Yy4mnrgWCDwG",
    "outputId": "38fdaaaf-633d-42fa-c7ef-41da96e04367"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f582ccc87b8>,\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'batch_size': [10, 25, 32],\n",
       "                         'nb_epoch': [100, 250, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = build_classifier_grid_search()\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAdYZY7XRwT9"
   },
   "outputs": [],
   "source": [
    "grid_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YigQxScaqpTt"
   },
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPSqQpOFqpNb"
   },
   "outputs": [],
   "source": [
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXj_ShIwtg46"
   },
   "source": [
    "PARAMETERS SELECTED BY RUNNING GRID SEARCH WERE :\n",
    "batch_size = 25\n",
    "nb_epoch = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLRundQ6xcHT"
   },
   "source": [
    "BUILDING THE ANN MODEL USING PARAMETERS SELECTED BY GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SCgxzN77wLKS",
    "outputId": "ce8cffe3-2855-4336-edd8-cfc7fa84ff54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.5647 - acc: 0.7951\n",
      "Epoch 2/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4390 - acc: 0.7960\n",
      "Epoch 3/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4300 - acc: 0.7960\n",
      "Epoch 4/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4256 - acc: 0.7960\n",
      "Epoch 5/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4220 - acc: 0.7960\n",
      "Epoch 6/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4191 - acc: 0.8132\n",
      "Epoch 7/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.4175 - acc: 0.8249\n",
      "Epoch 8/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4160 - acc: 0.8274\n",
      "Epoch 9/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4145 - acc: 0.8304\n",
      "Epoch 10/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4139 - acc: 0.8310\n",
      "Epoch 11/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4127 - acc: 0.8314\n",
      "Epoch 12/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4118 - acc: 0.8319\n",
      "Epoch 13/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4111 - acc: 0.8324\n",
      "Epoch 14/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4103 - acc: 0.8345\n",
      "Epoch 15/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4095 - acc: 0.8344\n",
      "Epoch 16/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4089 - acc: 0.8349\n",
      "Epoch 17/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.4082 - acc: 0.8344\n",
      "Epoch 18/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4077 - acc: 0.8350\n",
      "Epoch 19/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4073 - acc: 0.8352\n",
      "Epoch 20/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4063 - acc: 0.8342\n",
      "Epoch 21/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.4064 - acc: 0.8350\n",
      "Epoch 22/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4060 - acc: 0.8362\n",
      "Epoch 23/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4056 - acc: 0.8361\n",
      "Epoch 24/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4054 - acc: 0.8352\n",
      "Epoch 25/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4052 - acc: 0.8355\n",
      "Epoch 26/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4048 - acc: 0.8346\n",
      "Epoch 27/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4047 - acc: 0.8351\n",
      "Epoch 28/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.4042 - acc: 0.8352\n",
      "Epoch 29/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4039 - acc: 0.8342\n",
      "Epoch 30/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4040 - acc: 0.8347\n",
      "Epoch 31/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4035 - acc: 0.8355\n",
      "Epoch 32/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.4032 - acc: 0.8359\n",
      "Epoch 33/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4033 - acc: 0.8337\n",
      "Epoch 34/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4032 - acc: 0.8356\n",
      "Epoch 35/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4033 - acc: 0.8356\n",
      "Epoch 36/500\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.4029 - acc: 0.8354\n",
      "Epoch 37/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4030 - acc: 0.8359\n",
      "Epoch 38/500\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.4025 - acc: 0.8355\n",
      "Epoch 39/500\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.4025 - acc: 0.8357\n",
      "Epoch 40/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4023 - acc: 0.8355\n",
      "Epoch 41/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4021 - acc: 0.8346\n",
      "Epoch 42/500\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.4023 - acc: 0.8359\n",
      "Epoch 43/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.4022 - acc: 0.8349\n",
      "Epoch 44/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.4021 - acc: 0.8345\n",
      "Epoch 45/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4020 - acc: 0.8356\n",
      "Epoch 46/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4020 - acc: 0.8356\n",
      "Epoch 47/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4019 - acc: 0.8351\n",
      "Epoch 48/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4017 - acc: 0.8351\n",
      "Epoch 49/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.4014 - acc: 0.8351\n",
      "Epoch 50/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.4007 - acc: 0.8360\n",
      "Epoch 51/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4013 - acc: 0.8361\n",
      "Epoch 52/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4010 - acc: 0.8360\n",
      "Epoch 53/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4004 - acc: 0.8339\n",
      "Epoch 54/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4007 - acc: 0.8347\n",
      "Epoch 55/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4003 - acc: 0.8357\n",
      "Epoch 56/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3998 - acc: 0.8359\n",
      "Epoch 57/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3997 - acc: 0.8344\n",
      "Epoch 58/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3999 - acc: 0.8346\n",
      "Epoch 59/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3997 - acc: 0.8352\n",
      "Epoch 60/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3994 - acc: 0.8354\n",
      "Epoch 61/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3991 - acc: 0.8356\n",
      "Epoch 62/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3989 - acc: 0.8340\n",
      "Epoch 63/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3987 - acc: 0.8369\n",
      "Epoch 64/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3983 - acc: 0.8366\n",
      "Epoch 65/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3985 - acc: 0.8361\n",
      "Epoch 66/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3980 - acc: 0.8346\n",
      "Epoch 67/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3979 - acc: 0.8361\n",
      "Epoch 68/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3979 - acc: 0.8364\n",
      "Epoch 69/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3979 - acc: 0.8356\n",
      "Epoch 70/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3976 - acc: 0.8370\n",
      "Epoch 71/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3973 - acc: 0.8349\n",
      "Epoch 72/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3970 - acc: 0.8370\n",
      "Epoch 73/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3972 - acc: 0.8369\n",
      "Epoch 74/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3972 - acc: 0.8375\n",
      "Epoch 75/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3970 - acc: 0.8354\n",
      "Epoch 76/500\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 0.3964 - acc: 0.8361\n",
      "Epoch 77/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3966 - acc: 0.8361\n",
      "Epoch 78/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3967 - acc: 0.8367\n",
      "Epoch 79/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3965 - acc: 0.8351\n",
      "Epoch 80/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3965 - acc: 0.8370\n",
      "Epoch 81/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3963 - acc: 0.8362\n",
      "Epoch 82/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3959 - acc: 0.8369\n",
      "Epoch 83/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3960 - acc: 0.8360\n",
      "Epoch 84/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3960 - acc: 0.8361\n",
      "Epoch 85/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3959 - acc: 0.8367\n",
      "Epoch 86/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3957 - acc: 0.8360\n",
      "Epoch 87/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3956 - acc: 0.8375\n",
      "Epoch 88/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3958 - acc: 0.8364\n",
      "Epoch 89/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3956 - acc: 0.8364\n",
      "Epoch 90/500\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.3956 - acc: 0.8375\n",
      "Epoch 91/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3955 - acc: 0.8372\n",
      "Epoch 92/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3953 - acc: 0.8364\n",
      "Epoch 93/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3948 - acc: 0.8370\n",
      "Epoch 94/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3951 - acc: 0.8396\n",
      "Epoch 95/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3951 - acc: 0.8367\n",
      "Epoch 96/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3954 - acc: 0.8374\n",
      "Epoch 97/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3948 - acc: 0.8380\n",
      "Epoch 98/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3952 - acc: 0.8372\n",
      "Epoch 99/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3949 - acc: 0.8365\n",
      "Epoch 100/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3948 - acc: 0.8380\n",
      "Epoch 101/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3946 - acc: 0.8366\n",
      "Epoch 102/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3950 - acc: 0.8380\n",
      "Epoch 103/500\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 0.3949 - acc: 0.8361\n",
      "Epoch 104/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3943 - acc: 0.8370\n",
      "Epoch 105/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3948 - acc: 0.8372\n",
      "Epoch 106/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3946 - acc: 0.8366\n",
      "Epoch 107/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3942 - acc: 0.8360\n",
      "Epoch 108/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3947 - acc: 0.8375\n",
      "Epoch 109/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3946 - acc: 0.8367\n",
      "Epoch 110/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3942 - acc: 0.8374\n",
      "Epoch 111/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3945 - acc: 0.8374\n",
      "Epoch 112/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3944 - acc: 0.8380\n",
      "Epoch 113/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3946 - acc: 0.8365\n",
      "Epoch 114/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3941 - acc: 0.8367\n",
      "Epoch 115/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3942 - acc: 0.8380\n",
      "Epoch 116/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3941 - acc: 0.8366\n",
      "Epoch 117/500\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3944 - acc: 0.8379\n",
      "Epoch 118/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3941 - acc: 0.8367\n",
      "Epoch 119/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3940 - acc: 0.8365\n",
      "Epoch 120/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3941 - acc: 0.8384\n",
      "Epoch 121/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3941 - acc: 0.8374\n",
      "Epoch 122/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3945 - acc: 0.8371\n",
      "Epoch 123/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3939 - acc: 0.8367\n",
      "Epoch 124/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3941 - acc: 0.8376\n",
      "Epoch 125/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3938 - acc: 0.8387\n",
      "Epoch 126/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3940 - acc: 0.8367\n",
      "Epoch 127/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3940 - acc: 0.8376\n",
      "Epoch 128/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3938 - acc: 0.8352\n",
      "Epoch 129/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3939 - acc: 0.8369\n",
      "Epoch 130/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3936 - acc: 0.8375\n",
      "Epoch 131/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3939 - acc: 0.8372\n",
      "Epoch 132/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3935 - acc: 0.8374\n",
      "Epoch 133/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3936 - acc: 0.8365\n",
      "Epoch 134/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3936 - acc: 0.8379\n",
      "Epoch 135/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3935 - acc: 0.8374\n",
      "Epoch 136/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3927 - acc: 0.8374\n",
      "Epoch 137/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3940 - acc: 0.8376\n",
      "Epoch 138/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3938 - acc: 0.8377\n",
      "Epoch 139/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3932 - acc: 0.8386\n",
      "Epoch 140/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3941 - acc: 0.8375\n",
      "Epoch 141/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3932 - acc: 0.8367\n",
      "Epoch 142/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3936 - acc: 0.8375\n",
      "Epoch 143/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3937 - acc: 0.8374\n",
      "Epoch 144/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3933 - acc: 0.8389\n",
      "Epoch 145/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3936 - acc: 0.8370\n",
      "Epoch 146/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3936 - acc: 0.8372\n",
      "Epoch 147/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3933 - acc: 0.8377\n",
      "Epoch 148/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3934 - acc: 0.8372\n",
      "Epoch 149/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3936 - acc: 0.8375\n",
      "Epoch 150/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3934 - acc: 0.8380\n",
      "Epoch 151/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3934 - acc: 0.8387\n",
      "Epoch 152/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3932 - acc: 0.8365\n",
      "Epoch 153/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3936 - acc: 0.8375\n",
      "Epoch 154/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3934 - acc: 0.8370\n",
      "Epoch 155/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3930 - acc: 0.8366\n",
      "Epoch 156/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3936 - acc: 0.8381\n",
      "Epoch 157/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3934 - acc: 0.8386\n",
      "Epoch 158/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3932 - acc: 0.8381\n",
      "Epoch 159/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3930 - acc: 0.8380\n",
      "Epoch 160/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3933 - acc: 0.8384\n",
      "Epoch 161/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3932 - acc: 0.8385\n",
      "Epoch 162/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3935 - acc: 0.8372\n",
      "Epoch 163/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3934 - acc: 0.8389\n",
      "Epoch 164/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3930 - acc: 0.8369\n",
      "Epoch 165/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3933 - acc: 0.8371\n",
      "Epoch 166/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3934 - acc: 0.8380\n",
      "Epoch 167/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3934 - acc: 0.8364\n",
      "Epoch 168/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3926 - acc: 0.8366\n",
      "Epoch 169/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3929 - acc: 0.8382\n",
      "Epoch 170/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3934 - acc: 0.8392\n",
      "Epoch 171/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3931 - acc: 0.8392\n",
      "Epoch 172/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3930 - acc: 0.8375\n",
      "Epoch 173/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3930 - acc: 0.8389\n",
      "Epoch 174/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3930 - acc: 0.8381\n",
      "Epoch 175/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3931 - acc: 0.8382\n",
      "Epoch 176/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3932 - acc: 0.8375\n",
      "Epoch 177/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3927 - acc: 0.8380\n",
      "Epoch 178/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3932 - acc: 0.8384\n",
      "Epoch 179/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3931 - acc: 0.8370\n",
      "Epoch 180/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3932 - acc: 0.8386\n",
      "Epoch 181/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3931 - acc: 0.8377\n",
      "Epoch 182/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3929 - acc: 0.8380\n",
      "Epoch 183/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3933 - acc: 0.8384\n",
      "Epoch 184/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3929 - acc: 0.8371\n",
      "Epoch 185/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3924 - acc: 0.8389\n",
      "Epoch 186/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3932 - acc: 0.8401\n",
      "Epoch 187/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3928 - acc: 0.8386\n",
      "Epoch 188/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3929 - acc: 0.8371\n",
      "Epoch 189/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3925 - acc: 0.8375\n",
      "Epoch 190/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3933 - acc: 0.8379\n",
      "Epoch 191/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3932 - acc: 0.8370\n",
      "Epoch 192/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3928 - acc: 0.8384\n",
      "Epoch 193/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3929 - acc: 0.8384\n",
      "Epoch 194/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3930 - acc: 0.8372\n",
      "Epoch 195/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3929 - acc: 0.8380\n",
      "Epoch 196/500\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.3928 - acc: 0.8390\n",
      "Epoch 197/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3928 - acc: 0.8382\n",
      "Epoch 198/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3928 - acc: 0.8372\n",
      "Epoch 199/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3925 - acc: 0.8406\n",
      "Epoch 200/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3927 - acc: 0.8364\n",
      "Epoch 201/500\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3928 - acc: 0.8386\n",
      "Epoch 202/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3927 - acc: 0.8377\n",
      "Epoch 203/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3930 - acc: 0.8371\n",
      "Epoch 204/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3928 - acc: 0.8386\n",
      "Epoch 205/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3921 - acc: 0.8380\n",
      "Epoch 206/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3924 - acc: 0.8381\n",
      "Epoch 207/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3930 - acc: 0.8389\n",
      "Epoch 208/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3928 - acc: 0.8384\n",
      "Epoch 209/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3929 - acc: 0.8390\n",
      "Epoch 210/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3929 - acc: 0.8385\n",
      "Epoch 211/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3929 - acc: 0.8395\n",
      "Epoch 212/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3924 - acc: 0.8371\n",
      "Epoch 213/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3926 - acc: 0.8392\n",
      "Epoch 214/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3930 - acc: 0.8386\n",
      "Epoch 215/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3926 - acc: 0.8381\n",
      "Epoch 216/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3929 - acc: 0.8380\n",
      "Epoch 217/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3924 - acc: 0.8389\n",
      "Epoch 218/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3928 - acc: 0.8382\n",
      "Epoch 219/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3927 - acc: 0.8371\n",
      "Epoch 220/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3924 - acc: 0.8379\n",
      "Epoch 221/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3927 - acc: 0.8387\n",
      "Epoch 222/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3931 - acc: 0.8379\n",
      "Epoch 223/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3925 - acc: 0.8381\n",
      "Epoch 224/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3927 - acc: 0.8386\n",
      "Epoch 225/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3924 - acc: 0.8367\n",
      "Epoch 226/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3929 - acc: 0.8381\n",
      "Epoch 227/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3928 - acc: 0.8379\n",
      "Epoch 228/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3928 - acc: 0.8386\n",
      "Epoch 229/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3922 - acc: 0.8400\n",
      "Epoch 230/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3929 - acc: 0.8400\n",
      "Epoch 231/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3927 - acc: 0.8374\n",
      "Epoch 232/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3925 - acc: 0.8386\n",
      "Epoch 233/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3924 - acc: 0.8391\n",
      "Epoch 234/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3926 - acc: 0.8380\n",
      "Epoch 235/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3925 - acc: 0.8382\n",
      "Epoch 236/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3926 - acc: 0.8389\n",
      "Epoch 237/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3926 - acc: 0.8381\n",
      "Epoch 238/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3921 - acc: 0.8386\n",
      "Epoch 239/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3922 - acc: 0.8381\n",
      "Epoch 240/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3928 - acc: 0.8384\n",
      "Epoch 241/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3926 - acc: 0.8384\n",
      "Epoch 242/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3924 - acc: 0.8360\n",
      "Epoch 243/500\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 0.3929 - acc: 0.8380\n",
      "Epoch 244/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3921 - acc: 0.8397\n",
      "Epoch 245/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3924 - acc: 0.8384\n",
      "Epoch 246/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3921 - acc: 0.8399\n",
      "Epoch 247/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3924 - acc: 0.8385\n",
      "Epoch 248/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3926 - acc: 0.8387\n",
      "Epoch 249/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3923 - acc: 0.8374\n",
      "Epoch 250/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3926 - acc: 0.8387\n",
      "Epoch 251/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3920 - acc: 0.8396\n",
      "Epoch 252/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3922 - acc: 0.8384\n",
      "Epoch 253/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3920 - acc: 0.8376\n",
      "Epoch 254/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3921 - acc: 0.8386\n",
      "Epoch 255/500\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 0.3917 - acc: 0.8389\n",
      "Epoch 256/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3920 - acc: 0.8394\n",
      "Epoch 257/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3920 - acc: 0.8387\n",
      "Epoch 258/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3914 - acc: 0.8384\n",
      "Epoch 259/500\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.3921 - acc: 0.8386\n",
      "Epoch 260/500\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3919 - acc: 0.8390\n",
      "Epoch 261/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3914 - acc: 0.8396\n",
      "Epoch 262/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3915 - acc: 0.8415\n",
      "Epoch 263/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3913 - acc: 0.8399\n",
      "Epoch 264/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3914 - acc: 0.8386\n",
      "Epoch 265/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3911 - acc: 0.8409\n",
      "Epoch 266/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3908 - acc: 0.8411\n",
      "Epoch 267/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3907 - acc: 0.8407\n",
      "Epoch 268/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3902 - acc: 0.8415\n",
      "Epoch 269/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3895 - acc: 0.8425\n",
      "Epoch 270/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3891 - acc: 0.8421\n",
      "Epoch 271/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3880 - acc: 0.8415\n",
      "Epoch 272/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3878 - acc: 0.8421\n",
      "Epoch 273/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3864 - acc: 0.8421\n",
      "Epoch 274/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3845 - acc: 0.8419\n",
      "Epoch 275/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3823 - acc: 0.8445\n",
      "Epoch 276/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3789 - acc: 0.8436\n",
      "Epoch 277/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3759 - acc: 0.8444\n",
      "Epoch 278/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3725 - acc: 0.8455\n",
      "Epoch 279/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3691 - acc: 0.8470\n",
      "Epoch 280/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3664 - acc: 0.8461\n",
      "Epoch 281/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3643 - acc: 0.8491\n",
      "Epoch 282/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3623 - acc: 0.8487\n",
      "Epoch 283/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3604 - acc: 0.8500\n",
      "Epoch 284/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3577 - acc: 0.8529\n",
      "Epoch 285/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3561 - acc: 0.8550\n",
      "Epoch 286/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3546 - acc: 0.8551\n",
      "Epoch 287/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3526 - acc: 0.8590\n",
      "Epoch 288/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3518 - acc: 0.8582\n",
      "Epoch 289/500\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3506 - acc: 0.8580\n",
      "Epoch 290/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3499 - acc: 0.8575\n",
      "Epoch 291/500\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3491 - acc: 0.8585\n",
      "Epoch 292/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3486 - acc: 0.8590\n",
      "Epoch 293/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3480 - acc: 0.8585\n",
      "Epoch 294/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3479 - acc: 0.8606\n",
      "Epoch 295/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3476 - acc: 0.8585\n",
      "Epoch 296/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3471 - acc: 0.8616\n",
      "Epoch 297/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3468 - acc: 0.8612\n",
      "Epoch 298/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3468 - acc: 0.8595\n",
      "Epoch 299/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3460 - acc: 0.8595\n",
      "Epoch 300/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3465 - acc: 0.8597\n",
      "Epoch 301/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3457 - acc: 0.8616\n",
      "Epoch 302/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3466 - acc: 0.8595\n",
      "Epoch 303/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3460 - acc: 0.8599\n",
      "Epoch 304/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3461 - acc: 0.8614\n",
      "Epoch 305/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3460 - acc: 0.8609\n",
      "Epoch 306/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3458 - acc: 0.8616\n",
      "Epoch 307/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3454 - acc: 0.8604\n",
      "Epoch 308/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3452 - acc: 0.8607\n",
      "Epoch 309/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3454 - acc: 0.8605\n",
      "Epoch 310/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3458 - acc: 0.8581\n",
      "Epoch 311/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3452 - acc: 0.8609\n",
      "Epoch 312/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3450 - acc: 0.8607\n",
      "Epoch 313/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3453 - acc: 0.8616\n",
      "Epoch 314/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3454 - acc: 0.8614\n",
      "Epoch 315/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3450 - acc: 0.8609\n",
      "Epoch 316/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3447 - acc: 0.8617\n",
      "Epoch 317/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3448 - acc: 0.8610\n",
      "Epoch 318/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3449 - acc: 0.8610\n",
      "Epoch 319/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3441 - acc: 0.8615\n",
      "Epoch 320/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3453 - acc: 0.8617\n",
      "Epoch 321/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3447 - acc: 0.8606\n",
      "Epoch 322/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3451 - acc: 0.8587\n",
      "Epoch 323/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3443 - acc: 0.8600\n",
      "Epoch 324/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3444 - acc: 0.8602\n",
      "Epoch 325/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3444 - acc: 0.8596\n",
      "Epoch 326/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3446 - acc: 0.8592\n",
      "Epoch 327/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3436 - acc: 0.8594\n",
      "Epoch 328/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3443 - acc: 0.8626\n",
      "Epoch 329/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3447 - acc: 0.8600\n",
      "Epoch 330/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3443 - acc: 0.8615\n",
      "Epoch 331/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3446 - acc: 0.8600\n",
      "Epoch 332/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3441 - acc: 0.8630\n",
      "Epoch 333/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3434 - acc: 0.8610\n",
      "Epoch 334/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3434 - acc: 0.8601\n",
      "Epoch 335/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3444 - acc: 0.8595\n",
      "Epoch 336/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3442 - acc: 0.8594\n",
      "Epoch 337/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3443 - acc: 0.8592\n",
      "Epoch 338/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3440 - acc: 0.8595\n",
      "Epoch 339/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3441 - acc: 0.8609\n",
      "Epoch 340/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3431 - acc: 0.8614\n",
      "Epoch 341/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3438 - acc: 0.8624\n",
      "Epoch 342/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3440 - acc: 0.8624\n",
      "Epoch 343/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3433 - acc: 0.8611\n",
      "Epoch 344/500\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.3439 - acc: 0.8604\n",
      "Epoch 345/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3434 - acc: 0.8600\n",
      "Epoch 346/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3430 - acc: 0.8605\n",
      "Epoch 347/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3433 - acc: 0.8604\n",
      "Epoch 348/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3438 - acc: 0.8609\n",
      "Epoch 349/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3438 - acc: 0.8595\n",
      "Epoch 350/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3435 - acc: 0.8591\n",
      "Epoch 351/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3435 - acc: 0.8587\n",
      "Epoch 352/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3433 - acc: 0.8609\n",
      "Epoch 353/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3435 - acc: 0.8601\n",
      "Epoch 354/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3435 - acc: 0.8599\n",
      "Epoch 355/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3437 - acc: 0.8601\n",
      "Epoch 356/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3436 - acc: 0.8611\n",
      "Epoch 357/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3434 - acc: 0.8621\n",
      "Epoch 358/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3433 - acc: 0.8610\n",
      "Epoch 359/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3435 - acc: 0.8607\n",
      "Epoch 360/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3433 - acc: 0.8594\n",
      "Epoch 361/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3430 - acc: 0.8611\n",
      "Epoch 362/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3429 - acc: 0.8620\n",
      "Epoch 363/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3433 - acc: 0.8607\n",
      "Epoch 364/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3431 - acc: 0.8597\n",
      "Epoch 365/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3428 - acc: 0.8601\n",
      "Epoch 366/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3425 - acc: 0.8610\n",
      "Epoch 367/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3431 - acc: 0.8604\n",
      "Epoch 368/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3431 - acc: 0.8624\n",
      "Epoch 369/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3434 - acc: 0.8594\n",
      "Epoch 370/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3431 - acc: 0.8606\n",
      "Epoch 371/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3425 - acc: 0.8626\n",
      "Epoch 372/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3427 - acc: 0.8607\n",
      "Epoch 373/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3434 - acc: 0.8605\n",
      "Epoch 374/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3429 - acc: 0.8604\n",
      "Epoch 375/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3429 - acc: 0.8602\n",
      "Epoch 376/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3426 - acc: 0.8604\n",
      "Epoch 377/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3422 - acc: 0.8614\n",
      "Epoch 378/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3433 - acc: 0.8604\n",
      "Epoch 379/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3427 - acc: 0.8622\n",
      "Epoch 380/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3431 - acc: 0.8606\n",
      "Epoch 381/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3425 - acc: 0.8617\n",
      "Epoch 382/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3430 - acc: 0.8605\n",
      "Epoch 383/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3426 - acc: 0.8607\n",
      "Epoch 384/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3423 - acc: 0.8606\n",
      "Epoch 385/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3425 - acc: 0.8614\n",
      "Epoch 386/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3431 - acc: 0.8599\n",
      "Epoch 387/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3425 - acc: 0.8617\n",
      "Epoch 388/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3424 - acc: 0.8606\n",
      "Epoch 389/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3431 - acc: 0.8611\n",
      "Epoch 390/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3426 - acc: 0.8609\n",
      "Epoch 391/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3423 - acc: 0.8614\n",
      "Epoch 392/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3429 - acc: 0.8599\n",
      "Epoch 393/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3423 - acc: 0.8599\n",
      "Epoch 394/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3431 - acc: 0.8600\n",
      "Epoch 395/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3428 - acc: 0.8617\n",
      "Epoch 396/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3420 - acc: 0.8619\n",
      "Epoch 397/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3427 - acc: 0.8604\n",
      "Epoch 398/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3419 - acc: 0.8625\n",
      "Epoch 399/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3425 - acc: 0.8606\n",
      "Epoch 400/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3422 - acc: 0.8600\n",
      "Epoch 401/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3422 - acc: 0.8599\n",
      "Epoch 402/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3423 - acc: 0.8614\n",
      "Epoch 403/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3426 - acc: 0.8615\n",
      "Epoch 404/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3424 - acc: 0.8615\n",
      "Epoch 405/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3429 - acc: 0.8584\n",
      "Epoch 406/500\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.3424 - acc: 0.8597\n",
      "Epoch 407/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3414 - acc: 0.8625\n",
      "Epoch 408/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3420 - acc: 0.8627\n",
      "Epoch 409/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3422 - acc: 0.8622\n",
      "Epoch 410/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3423 - acc: 0.8604\n",
      "Epoch 411/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3422 - acc: 0.8617\n",
      "Epoch 412/500\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3425 - acc: 0.8612\n",
      "Epoch 413/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3423 - acc: 0.8611\n",
      "Epoch 414/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3419 - acc: 0.8612\n",
      "Epoch 415/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3422 - acc: 0.8596\n",
      "Epoch 416/500\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3416 - acc: 0.8599\n",
      "Epoch 417/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3416 - acc: 0.8586\n",
      "Epoch 418/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3422 - acc: 0.8594\n",
      "Epoch 419/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3420 - acc: 0.8615\n",
      "Epoch 420/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3415 - acc: 0.8609\n",
      "Epoch 421/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3419 - acc: 0.8599\n",
      "Epoch 422/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3417 - acc: 0.8611\n",
      "Epoch 423/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3421 - acc: 0.8614\n",
      "Epoch 424/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3415 - acc: 0.8609\n",
      "Epoch 425/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3415 - acc: 0.8601\n",
      "Epoch 426/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3423 - acc: 0.8602\n",
      "Epoch 427/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3420 - acc: 0.8605\n",
      "Epoch 428/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3419 - acc: 0.8587\n",
      "Epoch 429/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3417 - acc: 0.8619\n",
      "Epoch 430/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3412 - acc: 0.8605\n",
      "Epoch 431/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3423 - acc: 0.8592\n",
      "Epoch 432/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3421 - acc: 0.8591\n",
      "Epoch 433/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3418 - acc: 0.8587\n",
      "Epoch 434/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3420 - acc: 0.8610\n",
      "Epoch 435/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3413 - acc: 0.8599\n",
      "Epoch 436/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3418 - acc: 0.8595\n",
      "Epoch 437/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3420 - acc: 0.8614\n",
      "Epoch 438/500\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3418 - acc: 0.8595\n",
      "Epoch 439/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3420 - acc: 0.8611\n",
      "Epoch 440/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3417 - acc: 0.8600\n",
      "Epoch 441/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3415 - acc: 0.8591\n",
      "Epoch 442/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3420 - acc: 0.8589\n",
      "Epoch 443/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3417 - acc: 0.8622\n",
      "Epoch 444/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3419 - acc: 0.8609\n",
      "Epoch 445/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3420 - acc: 0.8611\n",
      "Epoch 446/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3415 - acc: 0.8606\n",
      "Epoch 447/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3417 - acc: 0.8619\n",
      "Epoch 448/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3424 - acc: 0.8600\n",
      "Epoch 449/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3418 - acc: 0.8611\n",
      "Epoch 450/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3412 - acc: 0.8616\n",
      "Epoch 451/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3411 - acc: 0.8591\n",
      "Epoch 452/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3417 - acc: 0.8601\n",
      "Epoch 453/500\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3417 - acc: 0.8605\n",
      "Epoch 454/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3417 - acc: 0.8609\n",
      "Epoch 455/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3411 - acc: 0.8596\n",
      "Epoch 456/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3417 - acc: 0.8611\n",
      "Epoch 457/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3413 - acc: 0.8621\n",
      "Epoch 458/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3416 - acc: 0.8602\n",
      "Epoch 459/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3418 - acc: 0.8602\n",
      "Epoch 460/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3414 - acc: 0.8607\n",
      "Epoch 461/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3414 - acc: 0.8602\n",
      "Epoch 462/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3413 - acc: 0.8615\n",
      "Epoch 463/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3418 - acc: 0.8602\n",
      "Epoch 464/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3416 - acc: 0.8604\n",
      "Epoch 465/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3414 - acc: 0.8599\n",
      "Epoch 466/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3409 - acc: 0.8611\n",
      "Epoch 467/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3411 - acc: 0.8611\n",
      "Epoch 468/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3417 - acc: 0.8601\n",
      "Epoch 469/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3407 - acc: 0.8601\n",
      "Epoch 470/500\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3410 - acc: 0.8595\n",
      "Epoch 471/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3416 - acc: 0.8604\n",
      "Epoch 472/500\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.3416 - acc: 0.8599\n",
      "Epoch 473/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3409 - acc: 0.8609\n",
      "Epoch 474/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3418 - acc: 0.8606\n",
      "Epoch 475/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3411 - acc: 0.8619\n",
      "Epoch 476/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3415 - acc: 0.8589\n",
      "Epoch 477/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3418 - acc: 0.8601\n",
      "Epoch 478/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3412 - acc: 0.8595\n",
      "Epoch 479/500\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.3413 - acc: 0.8604\n",
      "Epoch 480/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3414 - acc: 0.8606\n",
      "Epoch 481/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3412 - acc: 0.8606\n",
      "Epoch 482/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3414 - acc: 0.8596\n",
      "Epoch 483/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3413 - acc: 0.8612\n",
      "Epoch 484/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3414 - acc: 0.8617\n",
      "Epoch 485/500\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3414 - acc: 0.8584\n",
      "Epoch 486/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3411 - acc: 0.8600\n",
      "Epoch 487/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3411 - acc: 0.8589\n",
      "Epoch 488/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3412 - acc: 0.8612\n",
      "Epoch 489/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3405 - acc: 0.8620\n",
      "Epoch 490/500\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3413 - acc: 0.8595\n",
      "Epoch 491/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3410 - acc: 0.8620\n",
      "Epoch 492/500\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3409 - acc: 0.8601\n",
      "Epoch 493/500\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3415 - acc: 0.8605\n",
      "Epoch 494/500\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.3410 - acc: 0.8612\n",
      "Epoch 495/500\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3408 - acc: 0.8599\n",
      "Epoch 496/500\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3415 - acc: 0.8602\n",
      "Epoch 497/500\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3409 - acc: 0.8601\n",
      "Epoch 498/500\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3407 - acc: 0.8605\n",
      "Epoch 499/500\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3402 - acc: 0.8602\n",
      "Epoch 500/500\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.3413 - acc: 0.8615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f580b101f60>"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid = Sequential()\n",
    "clf_grid.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu', input_dim = 11))\n",
    "clf_grid.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu')) \n",
    "clf_grid.add(Dense(units = 1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "clf_grid.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "clf_grid.fit(X_train, y_train, batch_size = 25, epochs = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final accuracy obtained - 86.15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfepSoCgwLeo"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "clf_grid.save(\"FINAL_CLASSIFIER.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qajFh_9CVy-3"
   },
   "outputs": [],
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final Churn modelling ANN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
